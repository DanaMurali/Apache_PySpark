{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69637274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/darania/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/16 13:59:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"TestingRDDS\").getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be1bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = \"Spark makes life a lot easier and puts me into good Spirits, Spark is too Awesome!\".split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef70c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1e478e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spark', 'makes', 'life', 'a', 'lot', 'easier', 'and', 'puts', 'me', 'into', 'good', 'Spirits,', 'Spark', 'is', 'too', 'Awesome!']\n"
     ]
    }
   ],
   "source": [
    "print(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30b6174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_rdd = spark.sparkContext.parallelize(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14963240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 4) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "words_data = words_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "388eda2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark\n",
      "makes\n",
      "life\n",
      "a\n",
      "lot\n",
      "easier\n",
      "and\n",
      "puts\n",
      "me\n",
      "into\n",
      "good\n",
      "Spirits,\n",
      "Spark\n",
      "is\n",
      "too\n",
      "Awesome!\n"
     ]
    }
   ],
   "source": [
    "for word in words_data:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25cfff68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distinct alllows us to remove dupicates, e.g the word Spark.\n",
    "\n",
    "words_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "840ffc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06fa8546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark\n",
      "makes\n",
      "life\n",
      "a\n",
      "lot\n",
      "easier\n",
      "and\n",
      "puts\n",
      "me\n",
      "into\n",
      "good\n",
      "Spirits,\n",
      "Spark\n",
      "is\n",
      "too\n",
      "Awesome!\n"
     ]
    }
   ],
   "source": [
    "words_data = words_rdd.collect()\n",
    "for word in words_data:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5489bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_unique_rdd = words_rdd.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae7200c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "makes\n",
      "life\n",
      "a\n",
      "lot\n",
      "and\n",
      "puts\n",
      "Awesome!\n",
      "Spark\n",
      "into\n",
      "Spirits,\n",
      "is\n",
      "easier\n",
      "me\n",
      "too\n"
     ]
    }
   ],
   "source": [
    "for word in words_unique_rdd.collect():\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b50e55d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see if a word starts with S or not. Using python function called startswith.\n",
    "\n",
    "def wordStartsWith(word, letter):\n",
    "    return word.startswith(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f76c40a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark', 'Spirits,', 'Spark']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering records - normally use anonymous functions like lambda.\n",
    "# Using anonymous lambda function to test whether the word starts with an S.\n",
    "# Collecting it into a list.\n",
    "\n",
    "words_rdd.filter(lambda word: wordStartsWith(word, \"S\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "181efabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda word: wordStartsWith(word, \"S\")\n",
    "# The lambda variable is just word.\n",
    "# After the semi-colon we write any expression that can operate on the word variable.\n",
    "# As for the expression we have chosen to call a function that would return either true/false.\n",
    "# Dependeing on whether the match has been found or not.\n",
    "# If true (match is found), it is collected in a python list as shown in our output above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4b4e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map and Flat Map RDD Transformations\n",
    "\n",
    "# Map Transformation - Used to apply any complex operations like adding/updating column \n",
    "# or just transforming the data. The output of the map transformations would always have \n",
    "# the same number of records as inputs.\n",
    "\n",
    "# Example with list of numbers. For each number calculate the square and return the RDD that \n",
    "# contains a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53494973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "# Creating a list of numbers using range function.\n",
    "# Star added in front to unpack the range function.\n",
    "num_list = [*range(1, 21)]\n",
    "print(num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78dce75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD to hold the list of numbers\n",
    "nums_rdd = spark.sparkContext.parallelize(num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77dce3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rdd to store list of squared numbers as tuples (original number, squared value)\n",
    "nums_squared_rdd = nums_rdd.map(lambda n: (n, n*n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff431c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(2, 4)\n",
      "(3, 9)\n",
      "(4, 16)\n",
      "(5, 25)\n",
      "(6, 36)\n",
      "(7, 49)\n",
      "(8, 64)\n",
      "(9, 81)\n",
      "(10, 100)\n",
      "(11, 121)\n",
      "(12, 144)\n",
      "(13, 169)\n",
      "(14, 196)\n",
      "(15, 225)\n",
      "(16, 256)\n",
      "(17, 289)\n",
      "(18, 324)\n",
      "(19, 361)\n",
      "(20, 400)\n"
     ]
    }
   ],
   "source": [
    "for element in nums_squared_rdd.collect():\n",
    "    print (element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e729120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD that would store the transformed dataset of words starting with S:\n",
    "words_trd_rdd = words_rdd.map(lambda word: (word, word[0], wordStartsWith(word, \"S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7f2fcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Spark', 'S', True)\n",
      "('makes', 'm', False)\n",
      "('life', 'l', False)\n",
      "('a', 'a', False)\n",
      "('lot', 'l', False)\n",
      "('easier', 'e', False)\n",
      "('and', 'a', False)\n",
      "('puts', 'p', False)\n",
      "('me', 'm', False)\n",
      "('into', 'i', False)\n",
      "('good', 'g', False)\n",
      "('Spirits,', 'S', True)\n",
      "('Spark', 'S', True)\n",
      "('is', 'i', False)\n",
      "('too', 't', False)\n",
      "('Awesome!', 'A', False)\n"
     ]
    }
   ],
   "source": [
    "for element in words_trd_rdd.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13746ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat Map Transformation - Provides a simple extension of the map function. Sometimes you \n",
    "# may want to be able to take in a list of words and flatten the structure in a list of \n",
    "# letters. Can use flat map to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9bce276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'p', 'a', 'r', 'k', 'm', 'a', 'k', 'e', 's']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lambda function taking in each word to create a list of the word and printing out the first \n",
    "# 10 records. Taken the entire word list and then conveted it into a huge list of letters and\n",
    "# then printed out the first 10 elements.\n",
    "\n",
    "words_rdd.flatMap(lambda word: list(word)).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "181cae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting using SortByKey() transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e35765a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need a list of tuples that contain countries and some ranking. \n",
    "# SortByKey() tranformation requires a key value pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21cf79cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_list = [(\"India\", 91), (\"USA\", 4),(\"Greece\", 13)]\n",
    "\n",
    "# Create RDD of countries list:\n",
    "countries_rdd = spark.sparkContext.parallelize(countries_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9e67a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list from a sorted RDD:\n",
    "\n",
    "sorted_countries_list = countries_rdd.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1967561e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Greece', 13)\n",
      "('India', 91)\n",
      "('USA', 4)\n"
     ]
    }
   ],
   "source": [
    "# Sorted the country list by country name. As the country is the key and the ranking is\n",
    "# the value.\n",
    "\n",
    "for country in sorted_countries_list:\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc1c10a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to sort by value/ranking of countries in descending order instead.\n",
    "\n",
    "sorted_countries_list = countries_rdd.map(lambda c: (c[1], c[0])).sortByKey(False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc46a653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 'India')\n",
      "(13, 'Greece')\n",
      "(4, 'USA')\n"
     ]
    }
   ],
   "source": [
    "for country in sorted_countries_list:\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3893ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now work with Spark Actions: Collect, Count and Take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a785614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce Action - used to reduce a RDD. By reduce we mean to aggregrate values into just \n",
    "# one value as a result. Given a set of numbers, can reduce the set of numbers to once \n",
    "# value by summing them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "daea23de",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list = [1,5,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ef9aafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# Create variable that will hold result:\n",
    "result = spark.sparkContext.parallelize(num_list).reduce(lambda x, y: x + y)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8be68309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation of lambda function above:\n",
    "\n",
    "def sumList(x,y):\n",
    "    print(x, y)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6908cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 5\n",
      "6 2\n",
      "8 7\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3 4\n"
     ]
    }
   ],
   "source": [
    "result = spark.sparkContext.parallelize(num_list).reduce(lambda x, y: sumList(x,y))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67d7aff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start testing if left word is greater than right word. And the reduce funtion should \n",
    "# give the largest word in our list of words.\n",
    "\n",
    "def wordLengthReducer(leftWord, rightWord):\n",
    "    if len(leftWord) > len(rightWord):\n",
    "        return leftWord\n",
    "    else:\n",
    "        return rightWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9db3b22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Awesome!'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd.reduce(wordLengthReducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c078776b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spark'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the first elemnet of words_rdd\n",
    "words_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec9ad8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max value from a list of values:\n",
    "spark.sparkContext.parallelize(range(1,21)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1fe96866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Min value from a list of values:\n",
    "spark.sparkContext.parallelize(range(1,21)).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549d6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
